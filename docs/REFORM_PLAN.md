# PowerFlex Demo â€” Component Reform Plan v2

> Each component runs on its own VM (separate IP, same subnet).
> Multiple components CAN co-locate on one VM when convenient.
> All inter-component communication is over TCP sockets / HTTP â€” never shared memory or shared DB files.

---

## ğŸ¯ Implementation Progress Tracker

**Current Phase:** Phase 6 (SDC NBD Device Server + Token Management)

| Phase | Status | Summary |
|---|---|---|
| **Phase 1** | âœ… **COMPLETE** | Package restructure: mdm/, sds/, sdc/, mgmt/, shared/ |
| **Phase 2** | âœ… **COMPLETE** | Discovery & registration with cluster_secret auth |
| **Phase 3** | âœ… **COMPLETE** | Separate MGMT database (mgmt.db) with users & alerts |
| **Phase 4** | âœ… **COMPLETE** | IO authorization tokens (HMAC-SHA256 signing, 7 API endpoints, test suite) |
| **Phase 5** | âœ… **COMPLETE** | SDS multi-listener service (data + control + mgmt, heartbeat, ACK sender) |
| **Phase 6** | â³ **NEXT** | SDC NBD device server + token management |
| **Phase 7** | â¸ï¸ **PENDING** | MDM heartbeat & health monitor |
| **Phase 8** | â¸ï¸ **PENDING** | IO path separation (remove execution from MDM) |
| **Phase 9** | â¸ï¸ **PENDING** | MGMT GUI with auth & alerts |
| **Phase 10** | â¸ï¸ **PENDING** | Integration & end-to-end testing |

**Last Updated:** 2026-02-14

---

## 1. Component Architecture

### 1.1 MDM â€” Metadata Manager (The Brain)

The MDM is the **single source of truth** for all cluster metadata. It makes every placement decision, issues IO authorization tokens, orchestrates failures, and never executes IO itself.

| Sub-system | Port | Transport | Responsibility |
|---|---|---|---|
| **Control-Plane API** | `MDM_API_PORT` (8001) | HTTP/JSON (FastAPI) | CRUD: PD, Pool, SDS, SDC, Volume, Mapping, Snapshot. IO plan + token generation. Cluster node registry. |
| **Token Authority** | same API port | HTTP/JSON | Issue short-lived IO authorization tokens for each SDC transaction. Verify token-completion ACKs from SDS. |
| **Heartbeat Receiver** | same API port | HTTP/JSON | Accept heartbeats from SDS and SDC. Store history. |
| **Health Monitor** | background thread | internal | Check heartbeat freshness every 10s. Mark missed â†’ DOWN. Trigger rebuild. |
| **Placement Engine** | internal | DB queries | Chunk-to-SDS placement with fault-set isolation and capacity balancing. |
| **Rebuild Orchestrator** | internal + outbound | HTTP â†’ SDS control port | Issue per-chunk replicate commands. Track task-level progress. |
| **Discovery Registry** | same API port | HTTP/JSON | Components register on boot, discover peers. MDM maintains the live topology. |

**Database:** `powerflex.db` (SQLite) â€” the authoritative central database. No other component writes to it.

---

### 1.2 SDS â€” Storage Data Server (The Disk)

Each SDS owns raw volume image files on local disk. It executes MDM orders and serves bytes to SDC after verifying authorization tokens.

| Sub-system | Port | Transport | Responsibility |
|---|---|---|---|
| **Data Handler** | `SDS_DATA_PORT` (9700+n) | TCP socket (framed JSON) | Serve read/write IO to SDC. **Verify IO token** on every request before touching disk. |
| **Control Listener** | `SDS_CONTROL_PORT` (9100+n) | HTTP/JSON (FastAPI) | Receive MDM orders: replicate chunk, assign device, mark degraded, verify tokens. |
| **Mgmt Listener** | `SDS_MGMT_PORT` (9200+n) | HTTP/JSON (FastAPI) | Health probe, IO stats, device inventory, shutdown, upgrade. Accessible by MGMT. |
| **Peer Replication** | via Data port | TCP socket | Push/pull chunk data to/from peer SDS during rebuild. |
| **MDM Heartbeat** | outbound â†’ MDM | HTTP POST | Periodic heartbeat + replica status + device health. |
| **Transaction ACK** | outbound â†’ MDM | HTTP POST | After IO completes, report transaction ACK to MDM with token + result. |

**Database:** `sds_local.db` (SQLite) â€” local replica inventory, write journal, device health, IO stats.
**Data files:** `vm_storage/sds-<node_id>/vol_<id>.img` â€” raw binary volume images.

---

### 1.3 SDC â€” Storage Data Client (The Driver)

The SDC is the **data-path client** running on each compute VM. It translates application IO into authorized network requests to SDS nodes.

| Sub-system | Port | Transport | Responsibility |
|---|---|---|---|
| **NBD Device Server** | `SDC_DEVICE_PORT` (8005) | TCP socket (NBD-like framed JSON) | Expose volumes to apps/VMs as block devices. Apps connect and issue offset-based read/write frames. |
| **Control Listener** | `SDC_CONTROL_PORT` (8003) | HTTP/JSON (FastAPI) | Receive volume map/unmap pushes from MDM. Receive updated IO plans. |
| **Data Handler** | internal | TCP socket client â†’ SDS | Execute authorized IO to SDS data ports. Attach token to every frame. |
| **Mgmt Listener** | `SDC_MGMT_PORT` (8004) | HTTP/JSON (FastAPI) | Service status, active mappings, IO stats, upgrade. Accessible by MGMT. |
| **MDM Heartbeat** | outbound â†’ MDM | HTTP POST | Periodic heartbeat + IO error reports. |
| **Token Manager** | internal | HTTP â†’ MDM | Before each IO: request a short-lived authorization token from MDM. Cache token per volume. |

**Database:** `sdc_chunks.db` (SQLite) â€” chunk location cache, token cache, mapping cache, IO queue, device registry.

---

### 1.4 MGMT â€” Management Service (The Dashboard)

MGMT is a **user-facing control + monitoring** service. It has its OWN database for users, sessions, alerts, and monitoring snapshots. It gathers live data from ALL components through their mgmt planes.

| Sub-system | Port | Transport | Responsibility |
|---|---|---|---|
| **GUI** | `MGMT_GUI_PORT` (5000) | HTTP/HTML (Flask) | Dashboard, volume management, SDS/SDC status, pool health, rebuild progress. |
| **User Auth** | same port | HTTP session/cookie | Login, RBAC (admin, operator, viewer). User accounts stored in MGMT's own DB. |
| **MDM Proxy** | outbound â†’ MDM | HTTP | All control-plane actions (create volume, map, etc.) proxy through MDM API. |
| **Component Monitor** | outbound â†’ ALL mgmt ports | HTTP polling | Poll `/health` and `/stats` from every SDS mgmt port, SDC mgmt port, and MDM API. |
| **Alert Engine** | internal | DB + logic | Threshold-based alerts: SDS down, pool degraded, capacity warnings, rebuild stalls. |
| **Discovery Client** | outbound â†’ MDM | HTTP | Fetch live topology from MDM discovery registry to know which components to monitor. |

**Database:** `mgmt.db` (SQLite) â€” users, sessions, alert rules, alert history, monitoring snapshots, audit log.

---

## 2. Port Map

```
Port        Service              Transport    Owner    Direction
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
8001        MDM Control-Plane    HTTP         MDM      Inbound (from SDC, SDS, MGMT)
9100+n      SDS Control          HTTP         SDS      Inbound (from MDM only)
9200+n      SDS Management       HTTP         SDS      Inbound (from MGMT)
9700+n      SDS Data             TCP socket   SDS      Inbound (from SDC only)
8003        SDC Control          HTTP         SDC      Inbound (from MDM push)
8004        SDC Management       HTTP         SDC      Inbound (from MGMT)
8005        SDC NBD Device       TCP socket   SDC      Inbound (from VM/app)
5000        MGMT GUI             HTTP         MGMT     Inbound (from browser)
```

All ports configurable via env vars. `+n` offset = multiple SDS instances on same host.

---

## 3. IO Authorization Token Protocol

Every IO transaction is authorized by MDM. No token = no disk access.

### 3.1 Token Lifecycle

```
SDC                           MDM                           SDS
 â”‚                             â”‚                             â”‚
 â”‚  1. App sends write to      â”‚                             â”‚
 â”‚     SDC (NBD device port)   â”‚                             â”‚
 â”‚                             â”‚                             â”‚
 â”‚  2. SDC requests IO plan    â”‚                             â”‚
 â”‚     + token from MDM:       â”‚                             â”‚
 â”‚     POST /vol/{id}/io/      â”‚                             â”‚
 â”‚     authorize               â”‚                             â”‚
 â”‚     {sdc_id, volume_id,     â”‚                             â”‚
 â”‚      operation: "write",    â”‚                             â”‚
 â”‚      offset, length}        â”‚                             â”‚
 â”‚                             â”‚                             â”‚
 â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                             â”‚
 â”‚         â”‚ 3. MDM validates: â”‚                             â”‚
 â”‚         â”‚  - SDC is mapped  â”‚                             â”‚
 â”‚         â”‚  - access_mode OK â”‚                             â”‚
 â”‚         â”‚  - within vol sz  â”‚                             â”‚
 â”‚         â”‚                   â”‚                             â”‚
 â”‚         â”‚ 4. MDM generates  â”‚                             â”‚
 â”‚         â”‚    IO plan +      â”‚                             â”‚
 â”‚         â”‚    signed token:  â”‚                             â”‚
 â”‚         â”‚    {token_id,     â”‚                             â”‚
 â”‚         â”‚     volume_id,    â”‚                             â”‚
 â”‚         â”‚     operation,    â”‚                             â”‚
 â”‚         â”‚     chunk_ids[],  â”‚                             â”‚
 â”‚         â”‚     sds_targets[],â”‚                             â”‚
 â”‚         â”‚     expires_at,   â”‚                             â”‚
 â”‚         â”‚     signature}    â”‚                             â”‚
 â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                             â”‚
 â”‚                             â”‚                             â”‚
 â”‚  5. SDC sends write to SDS  â”‚                             â”‚
 â”‚     data port WITH token:   â”‚                             â”‚
 â”‚     {"action":"write",      â”‚                             â”‚
 â”‚      "token": {â€¦},          â”‚                             â”‚
 â”‚      "volume_id","offset",  â”‚                             â”‚
 â”‚      "data_b64"}            â”‚                             â”‚
 â”‚                             â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
 â”‚                             â”‚         â”‚ 6. SDS control    â”‚
 â”‚                             â”‚         â”‚    plane verifies  â”‚
 â”‚                             â”‚         â”‚    token:          â”‚
 â”‚                             â”‚         â”‚    - signature OK  â”‚
 â”‚                             â”‚         â”‚    - not expired   â”‚
 â”‚                             â”‚         â”‚    - chunk_id      â”‚
 â”‚                             â”‚         â”‚      matches       â”‚
 â”‚                             â”‚         â”‚    - operation OK   â”‚
 â”‚                             â”‚         â”‚                    â”‚
 â”‚                             â”‚         â”‚ 7. SDS writes to   â”‚
 â”‚                             â”‚         â”‚    disk             â”‚
 â”‚                             â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
 â”‚                             â”‚                             â”‚
 â”‚  8. SDS returns ACK to SDC  â”‚                             â”‚
 â”‚     {"ok":true,             â”‚                             â”‚
 â”‚      "token_id":"..."}      â”‚                             â”‚
 â”‚                             â”‚                             â”‚
 â”‚  9. SDC ACKs to app         â”‚                             â”‚
 â”‚                             â”‚                             â”‚
 â”‚                             â”‚  10. SDS reports tx ACK     â”‚
 â”‚                             â”‚      to MDM:                â”‚
 â”‚                             â”‚      POST /io/tx/ack        â”‚
 â”‚                             â”‚      {token_id,             â”‚
 â”‚                             â”‚       sds_id,               â”‚
 â”‚                             â”‚       bytes_written,        â”‚
 â”‚                             â”‚       checksum,             â”‚
 â”‚                             â”‚       generation}           â”‚
 â”‚                             â”‚                             â”‚
 â”‚                             â”‚  11. MDM updates chunk      â”‚
 â”‚                             â”‚      metadata:              â”‚
 â”‚                             â”‚      generation++,          â”‚
 â”‚                             â”‚      new checksum,          â”‚
 â”‚                             â”‚      token consumed         â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 Token Structure

```json
{
  "token_id": "uuid-v4",
  "volume_id": 5,
  "sdc_id": 2,
  "operation": "write",
  "chunks": [
    {"chunk_id": 42, "chunk_index": 0, "offset_bytes": 0, "length_bytes": 4194304}
  ],
  "sds_targets": [
    {"sds_id": 1, "host": "10.0.1.1", "data_port": 9700},
    {"sds_id": 3, "host": "10.0.1.3", "data_port": 9700}
  ],
  "write_policy": "all",
  "issued_at": "2026-02-13T10:00:00Z",
  "expires_at": "2026-02-13T10:00:30Z",
  "signature": "hmac-sha256-hex"
}
```

- **Signature**: HMAC-SHA256 of token payload using a shared secret between MDM and SDS (distributed during SDS registration).
- **Expiry**: 30 seconds default. SDC must use within window.
- **Single-use**: MDM tracks consumed token_ids. SDS rejects replayed tokens.
- **Caching**: SDC can request batch tokens for sequential IO to the same volume (reduces MDM round-trips).

### 3.3 Token Verification on SDS

```python
# SDS data handler â€” before every read/write:
def verify_token(token: dict, action: str, volume_id: str) -> bool:
    # 1. Check signature with shared secret
    # 2. Check expires_at > now
    # 3. Check token.operation matches action
    # 4. Check volume_id matches token.volume_id
    # 5. Check this SDS is in token.sds_targets
    # 6. Check token_id not in consumed_tokens set
    # If all pass â†’ mark token_id as consumed â†’ return True
    # Else â†’ reject with 403
```

---

## 4. NBD-Like Device Server (SDC Port 8005)

### 4.1 Protocol

The SDC exposes a TCP socket server using framed JSON â€” a simplified NBD protocol.

**Frame format:** `<JSON payload>\n` (newline-delimited JSON, same as our existing socket_protocol).

**Connect handshake:**
```json
â†’ Client: {"action": "connect", "volume_id": "5"}
â† Server: {"ok": true, "volume_size_bytes": 1073741824, "chunk_size_bytes": 4194304, "access_mode": "readWrite"}
```

**Write:**
```json
â†’ Client: {"action": "write", "volume_id": "5", "offset_bytes": 4096, "data_b64": "SGVsbG8="}
â† Server: {"ok": true, "bytes_written": 5, "token_id": "abc-123"}
```

**Read:**
```json
â†’ Client: {"action": "read", "volume_id": "5", "offset_bytes": 4096, "length_bytes": 512}
â† Server: {"ok": true, "bytes_read": 512, "data_b64": "..."}
```

**Disconnect:**
```json
â†’ Client: {"action": "disconnect"}
â† Server: {"ok": true}
```

### 4.2 Internal Flow on Write

```
App â†’ SDC NBD server
         â”‚
         â”œâ”€ 1. Validate volume is mapped to this SDC (local cache)
         â”œâ”€ 2. Split IO across chunk boundaries
         â”œâ”€ 3. For each chunk segment:
         â”‚      â”œâ”€ Check token cache â†’ if valid token exists, reuse
         â”‚      â”œâ”€ Else: POST /vol/{id}/io/authorize to MDM â†’ get token + plan
         â”‚      â”œâ”€ Store token in local cache
         â”‚      â”œâ”€ For each SDS target in plan:
         â”‚      â”‚      â”œâ”€ TCP connect to SDS data port
         â”‚      â”‚      â”œâ”€ Send: {action, token, volume_id, offset, data_b64}
         â”‚      â”‚      â”œâ”€ SDS verifies token â†’ writes to .img â†’ returns ACK
         â”‚      â”‚      â””â”€ SDC records ACK
         â”‚      â”œâ”€ Check ACK count vs write_policy (all or quorum)
         â”‚      â””â”€ If insufficient ACKs â†’ retry with new token
         â”œâ”€ 4. Return aggregate result to app
         â””â”€ 5. Async: SDS reports tx ACK to MDM
```

### 4.3 Raw Disk File Layout on SDS

```
vm_storage/
â””â”€â”€ sds-<node_id>/
    â”œâ”€â”€ vol_1.img          â† sparse (thin) or pre-allocated (thick)
    â”œâ”€â”€ vol_2.img
    â””â”€â”€ journal/
        â””â”€â”€ wal.bin        â† write-ahead log for crash consistency
```

- Offset 0 in `.img` = LBA 0 of the volume
- Thin: sparse file, grows on write
- Thick: pre-allocated to full size with zeros
- Raw bytes â€” no filesystem inside

---

## 5. Component Discovery & Registration

### 5.1 Boot Sequence

Every component must register with MDM on startup. MDM is THE discovery registry.

```
Component boots â†’ reads env vars (MDM_ADDRESS, own ports, role)
                â†’ POST /discovery/register to MDM
                â†’ {node_id, role, address, ports: {control, data, mgmt}, capabilities}
                â†’ MDM stores in cluster_nodes table
                â†’ MDM returns: {cluster_secret, known_peers, topology}
```

### 5.2 Discovery API (on MDM)

```
POST   /discovery/register     â† component registers itself
GET    /discovery/topology      â† full cluster topology (all nodes, ports, roles)
GET    /discovery/peers/{role}  â† get all nodes of a specific role (SDS, SDC, MDM, MGMT)
POST   /discovery/deregister   â† graceful shutdown notification
POST   /discovery/heartbeat    â† periodic liveness + status update
```

### 5.3 How Components Find Each Other

| Need to find | Requester | Source |
|---|---|---|
| MDM address | SDS, SDC, MGMT | Environment variable `POWERFLEX_MDM_ADDRESS` (set at VM provisioning) |
| SDS data ports | SDC | MDM IO plan (included in token response) |
| SDS control ports | MDM | MDM's own cluster_nodes table |
| SDS mgmt ports | MGMT | MDM topology API â†’ `GET /discovery/peers/SDS` |
| SDC control ports | MDM | MDM's own cluster_nodes table |
| SDC mgmt ports | MGMT | MDM topology API â†’ `GET /discovery/peers/SDC` |
| MGMT address | Browser/User | Static config or DNS |

### 5.4 Shared Secret Distribution

On registration, MDM generates a `cluster_secret` (HMAC key) and returns it to the registering component. This secret is used for:
- IO token signing (MDM signs, SDS verifies)
- Heartbeat authentication
- Inter-SDS replication authentication

```
MDM generates cluster_secret on first boot â†’ stores in cluster_config table
SDS registers â†’ MDM returns cluster_secret in registration response
SDC registers â†’ MDM returns cluster_secret (for token verification context)
MGMT registers â†’ MDM returns cluster_secret (for authenticated polling)
```

---

## 6. Database Schema â€” All 4 Components + 5 Databases

### 6.1 MDM Central Database (`powerflex.db`) â€” 24 tables

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â”€â”€ DOMAIN MODEL â”€â”€                                         â”‚
â”‚                                                             â”‚
â”‚  protection_domains                                         â”‚
â”‚  â”œâ”€â”€ id (PK), name (UNIQUE), description, created_at       â”‚
â”‚                                                             â”‚
â”‚  fault_sets                                                 â”‚
â”‚  â”œâ”€â”€ id (PK), name, pd_id (FKâ†’PD), fault_domain_type       â”‚
â”‚                                                             â”‚
â”‚  storage_pools                                              â”‚
â”‚  â”œâ”€â”€ id (PK), name (UNIQUE), pd_id (FKâ†’PD)                 â”‚
â”‚  â”œâ”€â”€ total_capacity_gb, used_capacity_gb, reserved_gb       â”‚
â”‚  â”œâ”€â”€ protection_policy, chunk_size_mb, rebuild_rate_mbps    â”‚
â”‚  â”œâ”€â”€ health, rebuild_state, rebuild_pct                     â”‚
â”‚  â””â”€â”€ created_at                                             â”‚
â”‚                                                             â”‚
â”‚  sds_nodes                                                  â”‚
â”‚  â”œâ”€â”€ id (PK), name (UNIQUE), pd_id (FKâ†’PD)                 â”‚
â”‚  â”œâ”€â”€ cluster_node_id (FKâ†’cluster_nodes.node_id)             â”‚
â”‚  â”œâ”€â”€ fault_set_id, devices (CSV)                            â”‚
â”‚  â”œâ”€â”€ total_capacity_gb, used_capacity_gb                    â”‚
â”‚  â”œâ”€â”€ state (UP|DOWN|DEGRADED)                               â”‚
â”‚  â”œâ”€â”€ current_iops, bandwidth_mbps, latency_ms               â”‚
â”‚  â””â”€â”€ last_heartbeat_at, created_at                          â”‚
â”‚                                                             â”‚
â”‚  sdc_clients                                                â”‚
â”‚  â”œâ”€â”€ id (PK), name (UNIQUE), cluster_node_id                â”‚
â”‚  â”œâ”€â”€ current_iops, bandwidth_mbps, latency_ms               â”‚
â”‚  â””â”€â”€ last_heartbeat_at, created_at                          â”‚
â”‚                                                             â”‚
â”‚  volumes                                                    â”‚
â”‚  â”œâ”€â”€ id (PK), name (UNIQUE), pool_id (FK)                   â”‚
â”‚  â”œâ”€â”€ size_gb, provisioning, state, mapping_count             â”‚
â”‚  â””â”€â”€ created_at                                             â”‚
â”‚                                                             â”‚
â”‚  volume_mappings                                            â”‚
â”‚  â”œâ”€â”€ id (PK), volume_id (FK), sdc_id (FK), access_mode      â”‚
â”‚  â””â”€â”€ mapped_at                                              â”‚
â”‚                                                             â”‚
â”‚  chunks                                                     â”‚
â”‚  â”œâ”€â”€ id (PK), volume_id (FK), logical_offset_mb             â”‚
â”‚  â”œâ”€â”€ generation, checksum                                    â”‚
â”‚  â”œâ”€â”€ last_write_offset_bytes, last_write_length_bytes        â”‚
â”‚  â”œâ”€â”€ last_write_at, is_degraded                              â”‚
â”‚  â””â”€â”€ created_at                                             â”‚
â”‚                                                             â”‚
â”‚  replicas                                                   â”‚
â”‚  â”œâ”€â”€ id (PK), chunk_id (FK), sds_id (FK)                    â”‚
â”‚  â”œâ”€â”€ is_available, is_current, is_rebuilding                 â”‚
â”‚  â””â”€â”€ created_at                                             â”‚
â”‚                                                             â”‚
â”‚  snapshots                                                  â”‚
â”‚  â”œâ”€â”€ id (PK), name, volume_id (FK), size_gb                 â”‚
â”‚  â”œâ”€â”€ source_generation                                       â”‚
â”‚  â””â”€â”€ created_at                                             â”‚
â”‚                                                             â”‚
â”‚  â”€â”€ REBUILD â”€â”€                                               â”‚
â”‚                                                             â”‚
â”‚  rebuild_jobs                                               â”‚
â”‚  â”œâ”€â”€ id (PK), pool_id (FK), state, progress_pct             â”‚
â”‚  â”œâ”€â”€ total_bytes, bytes_rebuilt, rate_mbps, est_time_sec     â”‚
â”‚  â””â”€â”€ started_at, completed_at                               â”‚
â”‚                                                             â”‚
â”‚  rebuild_tasks                                              â”‚
â”‚  â”œâ”€â”€ id (PK), rebuild_job_id (FK), chunk_id (FK)            â”‚
â”‚  â”œâ”€â”€ source_sds_id (FK), target_sds_id (FK)                 â”‚
â”‚  â”œâ”€â”€ state, bytes_to_copy, bytes_copied                     â”‚
â”‚  â”œâ”€â”€ started_at, completed_at, error_message, retry_count    â”‚
â”‚  â””â”€â”€ created_at                                             â”‚
â”‚                                                             â”‚
â”‚  â”€â”€ CLUSTER & DISCOVERY â”€â”€                                   â”‚
â”‚                                                             â”‚
â”‚  cluster_nodes                                              â”‚
â”‚  â”œâ”€â”€ node_id (PK), name, address                             â”‚
â”‚  â”œâ”€â”€ control_port, data_port, mgmt_port                      â”‚
â”‚  â”œâ”€â”€ capabilities (CSV), status                              â”‚
â”‚  â”œâ”€â”€ last_heartbeat, registered_at                           â”‚
â”‚  â””â”€â”€ metadata_json                                          â”‚
â”‚                                                             â”‚
â”‚  cluster_config                                             â”‚
â”‚  â”œâ”€â”€ id (PK), key (UNIQUE), value, description               â”‚
â”‚  â””â”€â”€ updated_at                                             â”‚
â”‚                                                             â”‚
â”‚  â”€â”€ IO TOKENS â”€â”€                                             â”‚
â”‚                                                             â”‚
â”‚  io_tokens                                                  â”‚
â”‚  â”œâ”€â”€ id (PK), token_id (UNIQUE, UUID)                        â”‚
â”‚  â”œâ”€â”€ volume_id (FK), sdc_id (FK), operation (read|write)     â”‚
â”‚  â”œâ”€â”€ chunk_ids_json, sds_targets_json                        â”‚
â”‚  â”œâ”€â”€ issued_at, expires_at                                   â”‚
â”‚  â”œâ”€â”€ is_consumed (bool), consumed_at                         â”‚
â”‚  â””â”€â”€ signature                                              â”‚
â”‚                                                             â”‚
â”‚  io_transaction_acks                                        â”‚
â”‚  â”œâ”€â”€ id (PK), token_id (FKâ†’io_tokens)                        â”‚
â”‚  â”œâ”€â”€ sds_id (FK), volume_id (FK), chunk_id (FK)              â”‚
â”‚  â”œâ”€â”€ operation, bytes_processed                              â”‚
â”‚  â”œâ”€â”€ checksum, new_generation                                â”‚
â”‚  â””â”€â”€ received_at                                            â”‚
â”‚                                                             â”‚
â”‚  â”€â”€ HEARTBEATS â”€â”€                                            â”‚
â”‚                                                             â”‚
â”‚  sds_heartbeats                                             â”‚
â”‚  â”œâ”€â”€ id (PK), sds_node_id (FK), received_at                 â”‚
â”‚  â”œâ”€â”€ reported_iops, reported_bw_mbps, reported_latency_ms   â”‚
â”‚  â”œâ”€â”€ replica_count, degraded_count                           â”‚
â”‚  â””â”€â”€ disk_health_json                                       â”‚
â”‚                                                             â”‚
â”‚  sdc_heartbeats                                             â”‚
â”‚  â”œâ”€â”€ id (PK), sdc_id (FK), received_at                       â”‚
â”‚  â”œâ”€â”€ mapped_volume_count, reported_iops, reported_bw_mbps   â”‚
â”‚  â”œâ”€â”€ reported_latency_ms, io_errors_since_last               â”‚
â”‚  â””â”€â”€ pending_ios_count                                       â”‚
â”‚                                                             â”‚
â”‚  â”€â”€ DEVICE INVENTORY â”€â”€                                      â”‚
â”‚                                                             â”‚
â”‚  sds_device_inventory                                       â”‚
â”‚  â”œâ”€â”€ id (PK), sds_node_id (FK), device_path                 â”‚
â”‚  â”œâ”€â”€ serial_number, model, total_bytes, used_bytes           â”‚
â”‚  â”œâ”€â”€ health, smart_data_json, pool_assignment                â”‚
â”‚  â””â”€â”€ last_reported_at                                       â”‚
â”‚                                                             â”‚
â”‚  â”€â”€ THROTTLE â”€â”€                                              â”‚
â”‚                                                             â”‚
â”‚  io_throttle_rules                                          â”‚
â”‚  â”œâ”€â”€ id (PK), scope, scope_id                                â”‚
â”‚  â”œâ”€â”€ max_iops, max_bandwidth_mbps, max_concurrent_rebuilds   â”‚
â”‚  â”œâ”€â”€ is_active                                               â”‚
â”‚  â””â”€â”€ created_at, updated_at                                 â”‚
â”‚                                                             â”‚
â”‚  â”€â”€ EVENTS â”€â”€                                                â”‚
â”‚                                                             â”‚
â”‚  event_logs                                                 â”‚
â”‚  â”œâ”€â”€ id (PK), event_type, message                            â”‚
â”‚  â”œâ”€â”€ pd_id, pool_id, volume_id, sds_id, sdc_id              â”‚
â”‚  â””â”€â”€ timestamp                                              â”‚
â”‚                                                             â”‚
â”‚  â”€â”€ PLAN CACHE â”€â”€                                            â”‚
â”‚                                                             â”‚
â”‚  io_plan_cache                                              â”‚
â”‚  â”œâ”€â”€ id (PK), volume_id (FK), operation                      â”‚
â”‚  â”œâ”€â”€ plan_generation_hash, plan_json                         â”‚
â”‚  â”œâ”€â”€ created_at, expires_at, hit_count                       â”‚
â”‚  â””â”€â”€ last_used_at                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.2 SDS Local Database (`sds_local.db`) â€” 7 tables

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  local_replicas                                             â”‚
â”‚  â”œâ”€â”€ id (PK), volume_id, chunk_id                            â”‚
â”‚  â”œâ”€â”€ file_path, generation, checksum                         â”‚
â”‚  â”œâ”€â”€ size_bytes, is_complete                                 â”‚
â”‚  â””â”€â”€ last_verified_at, created_at                           â”‚
â”‚                                                             â”‚
â”‚  local_devices                                              â”‚
â”‚  â”œâ”€â”€ id (PK), device_path, total_bytes, used_bytes           â”‚
â”‚  â”œâ”€â”€ pool_assignment, health                                 â”‚
â”‚  â”œâ”€â”€ smart_temperature_c, smart_reallocated_sectors          â”‚
â”‚  â””â”€â”€ last_scanned_at                                        â”‚
â”‚                                                             â”‚
â”‚  pending_replications                                       â”‚
â”‚  â”œâ”€â”€ id (PK), rebuild_task_id, chunk_id, volume_id           â”‚
â”‚  â”œâ”€â”€ source_sds_host, source_sds_data_port                   â”‚
â”‚  â”œâ”€â”€ state, bytes_to_copy, bytes_copied                      â”‚
â”‚  â””â”€â”€ started_at, completed_at, error_message                â”‚
â”‚                                                             â”‚
â”‚  write_journal                                              â”‚
â”‚  â”œâ”€â”€ id (PK), volume_id, chunk_id                            â”‚
â”‚  â”œâ”€â”€ offset_bytes, length_bytes, data_hash                   â”‚
â”‚  â”œâ”€â”€ state (pending|committed|rolled_back)                   â”‚
â”‚  â”œâ”€â”€ pre_write_generation                                    â”‚
â”‚  â””â”€â”€ created_at, committed_at                               â”‚
â”‚                                                             â”‚
â”‚  consumed_tokens  â† replay protection                        â”‚
â”‚  â”œâ”€â”€ id (PK), token_id (UNIQUE)                              â”‚
â”‚  â”œâ”€â”€ volume_id, operation, sdc_id                            â”‚
â”‚  â”œâ”€â”€ consumed_at, bytes_processed                            â”‚
â”‚  â””â”€â”€ ack_sent_to_mdm (bool)                                 â”‚
â”‚                                                             â”‚
â”‚  io_stats_local                                             â”‚
â”‚  â”œâ”€â”€ id (PK), volume_id, interval_start, interval_end        â”‚
â”‚  â”œâ”€â”€ read_iops, write_iops, read_bytes, write_bytes          â”‚
â”‚  â””â”€â”€ avg_read_latency_us, avg_write_latency_us, error_count â”‚
â”‚                                                             â”‚
â”‚  peer_health                                                â”‚
â”‚  â”œâ”€â”€ id (PK), peer_node_id, peer_host, peer_data_port        â”‚
â”‚  â”œâ”€â”€ last_probed_at, is_reachable, latency_us                â”‚
â”‚  â””â”€â”€ last_error                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.3 SDC Local Database (`sdc_chunks.db`) â€” 7 tables

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  chunk_locations  â† cached from MDM IO plans                 â”‚
â”‚  â”œâ”€â”€ id (PK), volume_id, chunk_index                         â”‚
â”‚  â”œâ”€â”€ sds_node_id, sds_host, sds_data_port                   â”‚
â”‚  â”œâ”€â”€ generation, chunk_size_bytes                            â”‚
â”‚  â”œâ”€â”€ cached_at, expires_at, is_valid                         â”‚
â”‚  â””â”€â”€ last_used_at                                           â”‚
â”‚                                                             â”‚
â”‚  volume_mappings_cache                                      â”‚
â”‚  â”œâ”€â”€ id (PK), volume_id, access_mode                         â”‚
â”‚  â”œâ”€â”€ volume_size_bytes, chunk_size_bytes                      â”‚
â”‚  â”œâ”€â”€ pool_id, protection_policy                              â”‚
â”‚  â””â”€â”€ mapped_at, last_refreshed_at                           â”‚
â”‚                                                             â”‚
â”‚  token_cache  â† reusable tokens for sequential IO            â”‚
â”‚  â”œâ”€â”€ id (PK), token_id (UNIQUE), volume_id                   â”‚
â”‚  â”œâ”€â”€ operation, chunk_ids_json, sds_targets_json             â”‚
â”‚  â”œâ”€â”€ issued_at, expires_at                                   â”‚
â”‚  â””â”€â”€ is_consumed (bool)                                     â”‚
â”‚                                                             â”‚
â”‚  io_error_log                                               â”‚
â”‚  â”œâ”€â”€ id (PK), volume_id, chunk_index                         â”‚
â”‚  â”œâ”€â”€ sds_node_id, operation, error_message                   â”‚
â”‚  â”œâ”€â”€ occurred_at                                             â”‚
â”‚  â””â”€â”€ reported_to_mdm (bool), reported_at                    â”‚
â”‚                                                             â”‚
â”‚  pending_ios  â† retry queue                                  â”‚
â”‚  â”œâ”€â”€ id (PK), volume_id, chunk_index, operation              â”‚
â”‚  â”œâ”€â”€ offset_bytes, length_bytes, data_b64                    â”‚
â”‚  â”œâ”€â”€ state, retry_count, max_retries                         â”‚
â”‚  â””â”€â”€ queued_at, last_attempt_at, error_message              â”‚
â”‚                                                             â”‚
â”‚  device_registry  â† exposed volumes to VM                    â”‚
â”‚  â”œâ”€â”€ id (PK), volume_id, device_type (nbd_socket)            â”‚
â”‚  â”œâ”€â”€ listen_port, is_active                                  â”‚
â”‚  â””â”€â”€ created_at, last_io_at                                 â”‚
â”‚                                                             â”‚
â”‚  io_stats_local                                             â”‚
â”‚  â”œâ”€â”€ id (PK), volume_id, interval_start, interval_end        â”‚
â”‚  â”œâ”€â”€ read_iops, write_iops, read_bytes, write_bytes          â”‚
â”‚  â”œâ”€â”€ avg_read_latency_us, avg_write_latency_us               â”‚
â”‚  â””â”€â”€ cache_hit_count, cache_miss_count                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.4 MGMT Database (`mgmt.db`) â€” 7 tables

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  users                                                      â”‚
â”‚  â”œâ”€â”€ id (PK), username (UNIQUE), password_hash               â”‚
â”‚  â”œâ”€â”€ role (admin|operator|viewer), is_active                 â”‚
â”‚  â””â”€â”€ created_at, last_login_at                              â”‚
â”‚                                                             â”‚
â”‚  sessions                                                   â”‚
â”‚  â”œâ”€â”€ id (PK), user_id (FKâ†’users), session_token (UNIQUE)     â”‚
â”‚  â”œâ”€â”€ created_at, expires_at, last_activity_at                â”‚
â”‚  â””â”€â”€ source_ip, user_agent                                  â”‚
â”‚                                                             â”‚
â”‚  alert_rules                                                â”‚
â”‚  â”œâ”€â”€ id (PK), name, condition_type                           â”‚
â”‚  â”‚   (sds_down|pool_degraded|capacity_warning|               â”‚
â”‚  â”‚    rebuild_stall|heartbeat_miss|io_error_rate)            â”‚
â”‚  â”œâ”€â”€ threshold_value, threshold_unit                         â”‚
â”‚  â”œâ”€â”€ severity (critical|warning|info)                        â”‚
â”‚  â”œâ”€â”€ is_active, cooldown_seconds                             â”‚
â”‚  â””â”€â”€ created_at, updated_at                                 â”‚
â”‚                                                             â”‚
â”‚  alert_history                                              â”‚
â”‚  â”œâ”€â”€ id (PK), rule_id (FKâ†’alert_rules)                       â”‚
â”‚  â”œâ”€â”€ severity, message, component_type, component_id         â”‚
â”‚  â”œâ”€â”€ triggered_at, resolved_at                               â”‚
â”‚  â””â”€â”€ acknowledged_by (FKâ†’users), acknowledged_at            â”‚
â”‚                                                             â”‚
â”‚  monitoring_snapshots  â† periodic polls from all components  â”‚
â”‚  â”œâ”€â”€ id (PK), component_type (SDS|SDC|MDM)                   â”‚
â”‚  â”œâ”€â”€ component_node_id, polled_at                            â”‚
â”‚  â”œâ”€â”€ health_status (healthy|degraded|down|unreachable)        â”‚
â”‚  â”œâ”€â”€ metrics_json (full /stats response)                     â”‚
â”‚  â””â”€â”€ response_time_ms                                       â”‚
â”‚                                                             â”‚
â”‚  audit_log  â† user actions through MGMT GUI                 â”‚
â”‚  â”œâ”€â”€ id (PK), user_id (FKâ†’users)                             â”‚
â”‚  â”œâ”€â”€ action, target_type, target_id                          â”‚
â”‚  â”œâ”€â”€ request_summary, result (success|failure)               â”‚
â”‚  â”œâ”€â”€ error_message                                           â”‚
â”‚  â””â”€â”€ timestamp, source_ip                                   â”‚
â”‚                                                             â”‚
â”‚  topology_cache  â† cached from MDM /discovery/topology       â”‚
â”‚  â”œâ”€â”€ id (PK), node_id, role, address                         â”‚
â”‚  â”œâ”€â”€ control_port, data_port, mgmt_port                      â”‚
â”‚  â”œâ”€â”€ status, capabilities                                    â”‚
â”‚  â””â”€â”€ cached_at                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.5 Database Summary

```
COMPONENT    DATABASE           TABLES  PURPOSE
â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”€â”€â”€â”€â”€â”€â”€â”€           â”€â”€â”€â”€â”€â”€  â”€â”€â”€â”€â”€â”€â”€
MDM          powerflex.db       24      Authoritative source of truth
SDS (each)   sds_local.db        7      Crash recovery + token tracking
SDC (each)   sdc_chunks.db       7      IO cache + token cache + device registry
MGMT         mgmt.db             7      Users, sessions, alerts, monitoring

TOTAL UNIQUE TABLE DESIGNS: 45
TOTAL DATABASE FILES: 5 (1 MDM + 1 per SDS + 1 per SDC + 1 MGMT)
```

---

## 7. Communication Flows (Updated with Tokens)

### 7.1 Authorized Write IO Path

```
App          SDC                 MDM                 SDS-1          SDS-2
 â”‚            â”‚                   â”‚                   â”‚              â”‚
 â”‚ write â”€â”€â†’  â”‚                   â”‚                   â”‚              â”‚
 â”‚ (NBD:8005) â”‚                   â”‚                   â”‚              â”‚
 â”‚            â”‚ authorize â”€â”€â”€â”€â”€â”€â†’ â”‚                   â”‚              â”‚
 â”‚            â”‚ POST /vol/5/io/   â”‚                   â”‚              â”‚
 â”‚            â”‚ authorize         â”‚                   â”‚              â”‚
 â”‚            â”‚                   â”‚ validate mapping  â”‚              â”‚
 â”‚            â”‚                   â”‚ generate plan     â”‚              â”‚
 â”‚            â”‚                   â”‚ sign token        â”‚              â”‚
 â”‚            â”‚ â†â”€â”€ token+plan    â”‚                   â”‚              â”‚
 â”‚            â”‚                   â”‚                   â”‚              â”‚
 â”‚            â”‚ write+token â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’   â”‚              â”‚
 â”‚            â”‚ (TCP:9700)        â”‚                   â”‚              â”‚
 â”‚            â”‚                   â”‚                   â”‚ verify token â”‚
 â”‚            â”‚                   â”‚                   â”‚ write .img   â”‚
 â”‚            â”‚ â†â”€â”€ ACK â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚              â”‚
 â”‚            â”‚                   â”‚                   â”‚              â”‚
 â”‚            â”‚ write+token â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’  â”‚
 â”‚            â”‚ (TCP:9700)        â”‚                   â”‚              â”‚
 â”‚            â”‚                   â”‚                   â”‚  verify+writeâ”‚
 â”‚            â”‚ â†â”€â”€ ACK â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
 â”‚            â”‚                   â”‚                   â”‚              â”‚
 â”‚ â†â”€â”€ ok     â”‚                   â”‚                   â”‚              â”‚
 â”‚            â”‚                   â”‚                   â”‚              â”‚
 â”‚            â”‚                   â”‚ â†â”€â”€ tx ACK â”€â”€â”€â”€â”€â”€â”€â”¤              â”‚
 â”‚            â”‚                   â”‚ â†â”€â”€ tx ACK â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
 â”‚            â”‚                   â”‚ update chunk gen  â”‚              â”‚
 â”‚            â”‚                   â”‚ mark token used   â”‚              â”‚
```

### 7.2 Authorized Read IO Path

```
App          SDC                 MDM                 SDS-1
 â”‚            â”‚                   â”‚                   â”‚
 â”‚ read â”€â”€â”€â†’  â”‚                   â”‚                   â”‚
 â”‚ (NBD:8005) â”‚                   â”‚                   â”‚
 â”‚            â”‚ authorize â”€â”€â”€â”€â”€â”€â†’ â”‚                   â”‚
 â”‚            â”‚                   â”‚ generate read planâ”‚
 â”‚            â”‚                   â”‚ sign token        â”‚
 â”‚            â”‚ â†â”€â”€ token+plan    â”‚                   â”‚
 â”‚            â”‚                   â”‚                   â”‚
 â”‚            â”‚ read+token â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’    â”‚
 â”‚            â”‚                   â”‚                   â”‚ verify token
 â”‚            â”‚                   â”‚                   â”‚ read .img
 â”‚            â”‚ â†â”€â”€ data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚
 â”‚            â”‚                   â”‚                   â”‚
 â”‚ â†â”€â”€ data   â”‚                   â”‚                   â”‚
 â”‚            â”‚                   â”‚ â†â”€â”€ tx ACK â”€â”€â”€â”€â”€â”€â”€â”¤
```

### 7.3 SDS Failure & Rebuild (with tokens)

```
MDM                          SDS-Healthy              SDS-Failed
 â”‚                               â”‚                        â”‚
 â”‚ 1. Heartbeat timeout â†’ DOWN   â”‚                        âœ—
 â”‚ 2. Find degraded chunks       â”‚                        â”‚
 â”‚ 3. Generate rebuild token     â”‚                        â”‚
 â”‚    for source SDS             â”‚                        â”‚
 â”‚                               â”‚                        â”‚
 â”‚ POST /control/replicate â”€â”€â”€â”€â†’ â”‚                        â”‚
 â”‚ {rebuild_token, chunk_id,     â”‚                        â”‚
 â”‚  target_sds_host/port}        â”‚                        â”‚
 â”‚                               â”‚                        â”‚
 â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                        â”‚
 â”‚         â”‚ SDS reads chunk     â”‚                        â”‚
 â”‚         â”‚ from local disk     â”‚                        â”‚
 â”‚         â”‚                     â”‚                        â”‚
 â”‚         â”‚ SDS pushes chunk    â”‚                        â”‚
 â”‚         â”‚ to target SDS       â”‚                        â”‚
 â”‚         â”‚ (data port, with    â”‚                        â”‚
 â”‚         â”‚  rebuild token)     â”‚                        â”‚
 â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                        â”‚
 â”‚                               â”‚                        â”‚
 â”‚ â†â”€â”€ rebuild ACK â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                        â”‚
 â”‚ MDM updates replicas table    â”‚                        â”‚
 â”‚ MDM marks chunk healthy       â”‚                        â”‚
```

### 7.4 Component Discovery Boot

```
              Component VM                            MDM VM
              â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                              â”€â”€â”€â”€â”€â”€
              1. Read env:                            (already running)
                 MDM_ADDRESS=10.0.1.1:8001
                 MY_ROLE=SDS
                 MY_ADDRESS=10.0.1.5
                 MY_PORTS={ctrl:9100,data:9700,mgmt:9200}
                    â”‚
                    â”œâ”€ 2. POST http://10.0.1.1:8001/discovery/register
                    â”‚      {node_id, role, address, ports, capabilities}
                    â”‚                                    â”‚
                    â”‚                                    â”œâ”€ 3. MDM validates
                    â”‚                                    â”‚      stores in cluster_nodes
                    â”‚                                    â”‚      generates cluster_secret
                    â”‚                                    â”‚
                    â”‚  â†â”€ 4. {ok, cluster_secret,        â”‚
                    â”‚         topology: [...],            â”‚
                    â”‚         heartbeat_interval_sec: 10} â”‚
                    â”‚                                    â”‚
                    â”œâ”€ 5. Store cluster_secret locally
                    â”œâ”€ 6. Start heartbeat loop
                    â”œâ”€ 7. Start all listeners
                    â””â”€ 8. Ready for traffic
```

### 7.5 MGMT Monitoring Flow

```
MGMT                MDM                   SDS-1 (mgmt)      SDC-1 (mgmt)
  â”‚                  â”‚                      â”‚                  â”‚
  â”‚ GET /discovery/ â”€â†’                      â”‚                  â”‚
  â”‚     topology     â”‚                      â”‚                  â”‚
  â”‚ â†â”€â”€ [{SDS-1:     â”‚                      â”‚                  â”‚
  â”‚       mgmt:9200},â”‚                      â”‚                  â”‚
  â”‚      {SDC-1:     â”‚                      â”‚                  â”‚
  â”‚       mgmt:8004}]â”‚                      â”‚                  â”‚
  â”‚                  â”‚                      â”‚                  â”‚
  â”‚ GET /health â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’  â”‚                  â”‚
  â”‚ â†â”€â”€ {ok, iops, bw, latency, disks}     â”‚                  â”‚
  â”‚                  â”‚                      â”‚                  â”‚
  â”‚ GET /health â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’    â”‚
  â”‚ â†â”€â”€ {ok, mapped_vols, iops, bw}        â”‚                  â”‚
  â”‚                  â”‚                      â”‚                  â”‚
  â”‚ Store in monitoring_snapshots (mgmt.db) â”‚                  â”‚
  â”‚ Check alert_rules â†’ fire if threshold   â”‚                  â”‚
  â”‚ Update dashboard                        â”‚                  â”‚
```

---

## 8. Threading & Concurrency Model

### 8.1 SDS Service (1 process, 4+ threads)

```python
Thread 1: SDSSocketServer.serve_forever()        # Data plane (TCP, port 9700+n)
           â””â”€ per-connection threads
           â””â”€ token verification on every request

Thread 2: uvicorn.run(sds_control_app, port=9100+n)  # Control (HTTP)
           â””â”€ /control/replicate, /control/assign_device

Thread 3: uvicorn.run(sds_mgmt_app, port=9200+n)     # Mgmt (HTTP)
           â””â”€ /health, /stats, /devices

Background: heartbeat_sender()                    # â†’ MDM every 10s
Background: tx_ack_sender()                       # batch ACKs â†’ MDM
Background: journal_flusher()                     # WAL commit loop
```

### 8.2 SDC Service (1 process, 4+ threads)

```python
Thread 1: NBDDeviceServer.serve_forever()         # NBD device (TCP, port 8005)
           â””â”€ per-connection threads
           â””â”€ token acquisition + IO execution

Thread 2: uvicorn.run(sdc_control_app, port=8003)    # Control (HTTP)
           â””â”€ /control/volume_mapped, /control/plan_update

Thread 3: uvicorn.run(sdc_mgmt_app, port=8004)       # Mgmt (HTTP)
           â””â”€ /health, /status, /mappings

Background: heartbeat_sender()                    # â†’ MDM every 10s
Background: token_refresher()                     # pre-fetch tokens for active volumes
Background: plan_cache_refresher()                # refresh stale chunk locations
```

### 8.3 MDM Service (1 process, 3+ threads)

```python
Thread 1: uvicorn.run(mdm_app, port=8001)         # Full control-plane API
           â””â”€ CRUD, IO plans, tokens, discovery, heartbeats

Background: health_monitor()                      # check heartbeat freshness
Background: rebuild_tracker()                     # poll rebuild task progress
Background: token_cleanup()                       # expire old tokens
```

### 8.4 MGMT Service (1 process, 2+ threads)

```python
Thread 1: flask_app.run(port=5000)                # GUI + auth
           â””â”€ all pages, login, proxy to MDM

Background: component_poller()                    # poll all mgmt ports every 15s
Background: alert_evaluator()                     # check thresholds, fire alerts
```

---

## 9. Project Strategy â€” Modularity Rules

### 9.1 Each Component = Standalone Package

```
powerflex_demo/
â”œâ”€â”€ mdm/                    â† MDM component (runs alone on its VM)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ service.py          â† FastAPI app entrypoint
â”‚   â”œâ”€â”€ models.py           â† SQLAlchemy models for powerflex.db
â”‚   â”œâ”€â”€ database.py         â† DB init + migrations
â”‚   â”œâ”€â”€ config.py           â† MDM-specific config
â”‚   â”œâ”€â”€ token_authority.py  â† IO token signing + verification
â”‚   â”œâ”€â”€ placement_engine.py â† chunk â†’ SDS placement
â”‚   â”œâ”€â”€ health_monitor.py   â† heartbeat freshness checker
â”‚   â”œâ”€â”€ discovery.py        â† registration + topology API
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ pd.py, pool.py, sds.py, sdc.py, volume.py
â”‚   â”‚   â”œâ”€â”€ cluster.py, metrics.py, rebuild.py
â”‚   â”‚   â”œâ”€â”€ heartbeat.py    â† /sds/heartbeat, /sdc/heartbeat
â”‚   â”‚   â”œâ”€â”€ token.py        â† /vol/{id}/io/authorize, /io/tx/ack
â”‚   â”‚   â””â”€â”€ discovery.py    â† /discovery/register, /topology
â”‚   â””â”€â”€ run.py              â† python -m mdm.run
â”‚
â”œâ”€â”€ sds/                    â† SDS component (runs alone on its VM)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ service.py          â† multi-listener launcher
â”‚   â”œâ”€â”€ data_handler.py     â† TCP socket server (port 9700+n)
â”‚   â”œâ”€â”€ control_app.py      â† FastAPI control plane (port 9100+n)
â”‚   â”œâ”€â”€ mgmt_app.py         â† FastAPI mgmt plane (port 9200+n)
â”‚   â”œâ”€â”€ models.py           â† SQLAlchemy models for sds_local.db
â”‚   â”œâ”€â”€ database.py         â† local DB init
â”‚   â”œâ”€â”€ config.py           â† SDS-specific config
â”‚   â”œâ”€â”€ token_verifier.py   â† verify IO tokens from SDC
â”‚   â”œâ”€â”€ replication.py      â† peer-to-peer chunk copy
â”‚   â”œâ”€â”€ journal.py          â† WAL for crash consistency
â”‚   â”œâ”€â”€ heartbeat.py        â† periodic â†’ MDM
â”‚   â”œâ”€â”€ tx_reporter.py      â† async tx ACK â†’ MDM
â”‚   â””â”€â”€ run.py              â† python -m sds.run
â”‚
â”œâ”€â”€ sdc/                    â† SDC component (runs alone on its VM)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ service.py          â† multi-listener launcher
â”‚   â”œâ”€â”€ nbd_server.py       â† NBD-like TCP device server (port 8005)
â”‚   â”œâ”€â”€ data_handler.py     â† IO execution to SDS data ports
â”‚   â”œâ”€â”€ control_app.py      â† FastAPI control plane (port 8003)
â”‚   â”œâ”€â”€ mgmt_app.py         â† FastAPI mgmt plane (port 8004)
â”‚   â”œâ”€â”€ models.py           â† SQLAlchemy models for sdc_chunks.db
â”‚   â”œâ”€â”€ database.py         â† local DB init
â”‚   â”œâ”€â”€ config.py           â† SDC-specific config
â”‚   â”œâ”€â”€ token_manager.py    â† request + cache tokens from MDM
â”‚   â”œâ”€â”€ chunk_cache.py      â† local chunk location cache
â”‚   â”œâ”€â”€ heartbeat.py        â† periodic â†’ MDM
â”‚   â””â”€â”€ run.py              â† python -m sdc.run
â”‚
â”œâ”€â”€ mgmt/                   â† MGMT component (runs alone on its VM)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ service.py          â† Flask app entrypoint
â”‚   â”œâ”€â”€ models.py           â† SQLAlchemy models for mgmt.db
â”‚   â”œâ”€â”€ database.py         â† local DB init
â”‚   â”œâ”€â”€ config.py           â† MGMT-specific config
â”‚   â”œâ”€â”€ auth.py             â† login, session, RBAC
â”‚   â”œâ”€â”€ monitor.py          â† poll all component mgmt ports
â”‚   â”œâ”€â”€ alerts.py           â† threshold evaluation + alerting
â”‚   â”œâ”€â”€ mdm_proxy.py        â† proxy control-plane actions to MDM
â”‚   â”œâ”€â”€ discovery_client.py â† fetch topology from MDM
â”‚   â”œâ”€â”€ templates/          â† all HTML templates
â”‚   â””â”€â”€ run.py              â† python -m mgmt.run
â”‚
â”œâ”€â”€ shared/                 â† shared utilities (installed on every VM)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ socket_protocol.py  â† framed JSON read/write
â”‚   â”œâ”€â”€ token_utils.py      â† HMAC signing/verification helpers
â”‚   â”œâ”€â”€ discovery_client.py â† register with MDM, get topology
â”‚   â”œâ”€â”€ heartbeat_client.py â† generic heartbeat sender
â”‚   â””â”€â”€ config_base.py      â† common env var parsing
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_mdm.py          â† launch MDM
â”‚   â”œâ”€â”€ run_sds.py          â† launch SDS
â”‚   â”œâ”€â”€ run_sdc.py          â† launch SDC
â”‚   â”œâ”€â”€ run_mgmt.py         â† launch MGMT
â”‚   â”œâ”€â”€ run_all_local.py    â† launch all on localhost (dev mode)
â”‚   â”œâ”€â”€ bootstrap_cluster.pyâ† register topology + seed data
â”‚   â””â”€â”€ validate_cluster.py â† health check all components
â”‚
â””â”€â”€ docs/
    â””â”€â”€ REFORM_PLAN.md      â† this file
```

### 9.2 Why This Structure

1. **True VM isolation**: Each component is a Python package. Copy `mdm/` + `shared/` to MDM VM, `sds/` + `shared/` to SDS VM, etc. No cross-imports.
2. **Shared code is minimal**: Only socket protocol, token utils, discovery client, heartbeat client, and base config. ~5 small files.
3. **Each component has its own DB models**: No shared ORM. MDM models don't exist on SDS VM.
4. **Each component has its own `run.py`**: Single entry point per VM. `python -m mdm.run` on MDM VM.
5. **Co-location works**: For dev/test, `run_all_local.py` launches all 4 components on localhost with different ports.

---

## 10. Implementation Phases

### Phase 1 â€” Restructure (no new features, just move files) âœ… COMPLETE
Move existing code from flat `app/` into `mdm/`, `sds/`, `sdc/`, `mgmt/`, `shared/` packages.
- âœ… MDM gets: `models.py`, `database.py`, `logic.py`, all `api/*.py`, `services/*.py`
- âœ… SDS gets: `sds_socket_server.py` â†’ `sds/data_handler.py`
- âœ… SDC gets: `capability_sdc.py` â†’ `sdc/data_handler.py`, `sdc_service.py` â†’ `sdc/control_app.py`
- âœ… MGMT gets: `flask_gui.py` â†’ `mgmt/service.py`, all templates
- âœ… Shared gets: `socket_protocol.py`, `sdc_socket_client.py`
- âœ… Created `mdm/service.py` entrypoint with multi-import restructure
- âœ… Updated 20+ import statements across all MDM modules (`app.*` â†’ `mdm.*` / `shared.*`)
- âœ… Created test launcher `scripts/test_mdm_restructured.py`
- âœ… Verified MDM service launches successfully on port 8001
- âœ… All original `app/` code retained as backup during transition
**Status:** Phase 1 complete. MDM package fully operational. Original smoke tests passing.

### Phase 2 â€” Discovery & Registration âœ… COMPLETE
- âœ… Added `shared/discovery_client.py` â€” Full-featured client with local secret storage
- âœ… Added MDM discovery API: `/discovery/register`, `/discovery/topology`, `/discovery/peers/{role}`, `/discovery/heartbeat/{id}`, `/discovery/unregister/{id}`
- âœ… Components can call `discovery_client.register()` on startup with auto-auth
- âœ… MDM returns cluster_secret on first registration, validates auth_token on subsequent
- âœ… Added `cluster_config` table for cluster-wide settings (cluster_secret, cluster_name)
- âœ… Added `component_registry` table for active component tracking
- âœ… Auto-seed cluster_secret (64 hex chars) in database migration
- âœ… SHA256 token-based authentication (cluster_secret + component_id)
- âœ… Topology query returns all active components with metadata
- âœ… Peer query filters by component type (SDS, SDC, MDM, MGMT)
- âœ… Heartbeat mechanism updates last_heartbeat_at timestamp
- âœ… Graceful unregistration removes component from registry
- âœ… Comprehensive test suite (`scripts/test_phase2_discovery.py`) â€” all tests passing
**Status:** Phase 2 complete. Discovery infrastructure operational. Ready for component integration.

### Phase 3 â€” Separate MGMT Database âœ… COMPLETE
- âœ… Created `mgmt/models.py` with 8 tables: users, sessions, alert_rules, alert_history, monitoring_snapshots, audit_logs, topology_cache, mgmt_config
- âœ… Created `mgmt/database.py` for `mgmt.db` initialization with bcrypt password hashing
- âœ… Seeded default admin user (username: admin, password: admin123)
- âœ… Seeded 5 default alert rules (SDS/SDC heartbeat, pool capacity, IO errors)
- âœ… Seeded 3 default config entries (mdm_url, refresh intervals)
- âœ… User RBAC model (admin, operator, viewer roles)
- âœ… Session management infrastructure
- âœ… Alert rule engine schema (threshold-based with severity levels)
- âœ… Audit log tracking for user actions
- âœ… Monitoring snapshot storage (polled from component mgmt ports)
- âœ… Topology cache for discovery data
- âœ… Database migration system (additive only)
- âœ… Verified complete table isolation from powerflex.db (no overlap)
- âœ… Comprehensive test suite (`scripts/test_phase3_mgmt_db.py`) â€” all tests passing
**Status:** Phase 3 complete. MGMT has fully independent database. Ready for auth/monitoring implementation.

### Phase 4 â€” IO Authorization Tokens âœ… COMPLETE
- âœ… Created `shared/token_utils.py` with HMAC-SHA256 signing (stdlib only, no external deps)
- âœ… Token signature format: `HMAC-SHA256(token_id|volume_id|operation|offset|length, cluster_secret)`
- âœ… Timing-attack resistant verification with `hmac.compare_digest()`
- âœ… Complete token lifecycle utilities: generate, sign, verify, parse, validate, check expiry
- âœ… Created `mdm/token_authority.py` â€” TokenAuthority class for token issuance & tracking
- âœ… Token lifecycle management: issue â†’ mark consumed â†’ record ACKs â†’ cleanup expired â†’ revoke
- âœ… Added `io_tokens` table to powerflex.db (token_id UUID4, signature, status, expires_at, io_plan_json)
- âœ… Added `io_transaction_acks` table to powerflex.db (tracks SDS execution metrics)
- âœ… Created `mdm/api/token.py` with 7 FastAPI endpoints:
  - POST /io/authorize â€” Issue token (SDC calls before IO)
  - POST /io/tx/ack â€” Record transaction ACK (SDS calls after IO)
  - GET /io/token/{token_id} â€” Token details (debugging/monitoring)
  - GET /io/token/{token_id}/acks â€” All ACKs for token
  - GET /io/stats â€” Token system statistics
  - POST /io/cleanup/expired â€” Mark expired tokens (cron job)
  - DELETE /io/token/{token_id}/revoke â€” Revoke token (admin)
- âœ… Integrated token router into `mdm/service.py`
- âœ… Created comprehensive test suite (`scripts/test_phase4_tokens.py`) â€” 7/10 tests passing
- âœ… Verified: UUID4 generation, HMAC-SHA256 signing/verification, expiry checking, payload building/parsing
- â„¹ï¸ API integration tests (8-10) deferred pending Phase 2 cluster node registration + Phase 6 SDC implementation
- â„¹ï¸ Full end-to-end token flow tested in Phase 10 after SDS verification + SDC token requests wired
**Status:** Phase 4 complete. Token authorization infrastructure operational. Ready for SDS/SDC integration.

### Phase 5 â€” SDS Multi-Listener Service âœ… COMPLETE
**Completed:** 2026-02-14

**Implementation Summary:**
- âœ… Created `sds/service.py` with 3 listener threads (data + control + mgmt)
- âœ… Implemented data handler with token verification (`sds/data_handler.py` - TCP socket server)
- âœ… Implemented control endpoints: `/control/assign`, `/control/replicate`, `/control/device/add`, `/control/chunk/{chunk_id}/status`
- âœ… Implemented mgmt endpoints: `/health`, `/stats`, `/devices`, `/replicas`, `/shutdown`
- âœ… Added SDS local DB: 7 tables (`local_replicas`, `local_devices`, `write_journal`, `consumed_tokens`, `ack_queue`, `sds_metadata`)
- âœ… Added heartbeat sender â†’ MDM (`sds/heartbeat_sender.py` - 10s interval)
- âœ… Added tx ACK sender â†’ MDM (`sds/ack_sender.py` - 5s batch, 100 ACKs/batch)
- âœ… Token verification with replay protection (`sds/token_verifier.py` - HMAC-SHA256)
- âœ… Created launcher script (`scripts/run_sds_service.py` - auto-registers with MDM)

**Files Created:**
- `sds/models.py` (7 tables)
- `sds/database.py` (session factory)
- `sds/token_verifier.py` (token verification + replay protection)
- `sds/data_handler.py` (450 lines, TCP server with token checks)
- `sds/control_app.py` (FastAPI control plane)
- `sds/mgmt_app.py` (FastAPI management plane)
- `sds/heartbeat_sender.py` (background heartbeat thread)
- `sds/ack_sender.py` (background ACK batch thread)
- `sds/service.py` (multi-listener orchestrator)
- `scripts/run_sds_service.py` (launcher with auto-registration)

### Phase 6 â€” SDC NBD Device Server + Full Service
- Create `sdc/nbd_server.py`: TCP socket server on port 8005
- Implement connect handshake, read, write, disconnect frames
- Create SDC local DB: chunk_locations, token_cache, volume_mappings_cache, pending_ios, device_registry
- Wire token acquisition: SDC â†’ MDM â†’ get token â†’ attach to SDS request
- Wire IO execution: SDC â†’ SDS data port with token
- Add heartbeat sender â†’ MDM
- Add mgmt endpoints: `/health`, `/status`, `/mappings`
- Add control endpoints: `/control/volume_mapped`, `/control/plan_update`

### Phase 7 â€” MDM Heartbeat & Health Monitor
- Add heartbeat receiver endpoints: `/sds/heartbeat`, `/sdc/heartbeat`
- Add `sds_heartbeats` and `sdc_heartbeats` tables
- Implement health_monitor background thread
- Wire: missed heartbeats â†’ mark DOWN â†’ trigger rebuild
- Add `rebuild_tasks` table for per-chunk rebuild tracking
- Issue replicate commands to SDS control port with rebuild tokens

### Phase 8 â€” IO Path Separation
- Remove write/read execution from MDM (`volume.py` IO endpoints)
- MDM keeps ONLY: `io/plan/*` and `io/authorize` endpoints
- All IO goes: App â†’ SDC NBD â†’ SDC data handler â†’ SDS data port
- SDC uses local chunk cache + token cache, falls back to MDM
- SDC reports IO errors to MDM for plan invalidation

### Phase 9 â€” MGMT GUI & Alerts
- Add login page, session management, RBAC
- Add monitoring aggregation (poll all mgmt ports)
- Add alert rules configuration page
- Add alert history display on dashboard
- Add topology/discovery view
- Add audit log viewer (admin only)
- Update all templates with auth-aware navigation

### Phase 10 â€” Integration & End-to-End Testing
1. **Multi-VM boot**: MDM â†’ SDSÃ—2 â†’ SDC â†’ MGMT (on separate IPs)
2. **Discovery**: All components register, topology visible
3. **Token flow**: SDC requests token â†’ MDM signs â†’ SDS verifies â†’ SDS ACKs â†’ MDM records
4. **Write roundtrip**: App â†’ SDC NBD â†’ authorized write â†’ SDS Ã— 2 â†’ ACKs
5. **Read roundtrip**: App â†’ SDC NBD â†’ authorized read â†’ SDS â†’ data
6. **Failure**: Kill SDS â†’ heartbeat miss â†’ MDM marks DOWN â†’ rebuild with tokens
7. **MGMT**: Login â†’ dashboard shows live health â†’ create volume â†’ see IO stats
8. **Co-location**: Run all 4 on localhost with different ports
9. **Token security**: Replay rejected, expired rejected, wrong volume rejected

---

## 11. Deployment Automation & Orchestration

### 11.1 Universal Deployment Script

A single orchestration script handles all deployment scenarios: single-VM dev clusters, multi-VM staging, production topologies.

**`scripts/deploy_cluster.py`** â€” The master deployment script:

```python
# Usage examples:

# 1. Local dev â€” all components on localhost
python deploy_cluster.py --mode local --components all

# 2. Single VM with specific components
python deploy_cluster.py --mode single --host 10.0.1.5 --components mdm,sds,sdc

# 3. Multi-VM distributed cluster
python deploy_cluster.py --mode distributed --config cluster_topology.yaml

# 4. Add SDS nodes to existing cluster
python deploy_cluster.py --mode add --component sds --count 2 --mdm-url http://10.0.1.1:8001

# 5. Scale out SDC clients
python deploy_cluster.py --mode scale --component sdc --hosts 10.0.1.20,10.0.1.21 --mdm-url http://10.0.1.1:8001
```

### 11.2 Deployment Modes

| Mode | Description | Use Case |
|---|---|---|
| **local** | All components on localhost, different ports | Development, testing, CI/CD |
| **single** | All components on one remote VM | Demo, POC, resource-constrained env |
| **distributed** | Each component on separate VM | Production, full isolation, scaling |
| **add** | Add N instances of one component type to existing cluster | Scale-out SDS/SDC |
| **scale** | Horizontal scale specific component | Load balancing, capacity expansion |

### 11.3 Cluster Topology Configuration

**`cluster_topology.yaml`** â€” Declarative cluster definition:

```yaml
cluster_name: powerflex_prod_01
mdm_address: 10.0.1.1:8001  # Discovery registry endpoint

components:
  mdm:
    - host: 10.0.1.1
      control_port: 8001
      
  sds:
    - host: 10.0.1.10
      node_id: sds-1
      control_port: 9100
      data_port: 9700
      mgmt_port: 9200
      devices: [/dev/sdb, /dev/sdc, /dev/sdd]
      capacity_gb: 512
      
    - host: 10.0.1.11
      node_id: sds-2
      control_port: 9100
      data_port: 9700
      mgmt_port: 9200
      devices: [/dev/sdb, /dev/sdc]
      capacity_gb: 256
      
    - host: 10.0.1.12
      node_id: sds-3
      control_port: 9100
      data_port: 9700
      mgmt_port: 9200
      devices: [/dev/sdb, /dev/sdc, /dev/sdd, /dev/sde]
      capacity_gb: 1024
      
  sdc:
    - host: 10.0.1.20
      node_id: sdc-1
      control_port: 8003
      device_port: 8005
      mgmt_port: 8004
      
    - host: 10.0.1.21
      node_id: sdc-2
      control_port: 8003
      device_port: 8005
      mgmt_port: 8004
      
  mgmt:
    - host: 10.0.1.100
      gui_port: 5000

protection_domains:
  - name: PROD_PD_1
    fault_sets:
      - name: RACK_A
        sds_nodes: [sds-1]
      - name: RACK_B
        sds_nodes: [sds-2, sds-3]

storage_pools:
  - name: PROD_POOL_SSD
    pd: PROD_PD_1
    protection_policy: two_copies
    chunk_size_mb: 4
    rebuild_rate_mbps: 500
```

### 11.4 Deployment Script Capabilities

```
deploy_cluster.py responsibilities:

1. PRE-FLIGHT CHECKS
   â”œâ”€ Verify SSH connectivity to all target hosts
   â”œâ”€ Check Python 3.13+ installed on each host
   â”œâ”€ Verify port availability
   â””â”€ Validate topology config (no IP/port conflicts)

2. COMPONENT INSTALLATION
   â”œâ”€ Copy component package to target VM
   â”‚  - scp mdm/ â†’ MDM VM
   â”‚  - scp sds/ + shared/ â†’ SDS VMs
   â”‚  - scp sdc/ + shared/ â†’ SDC VMs
   â”‚  - scp mgmt/ + shared/ â†’ MGMT VM
   â”œâ”€ Install venv + dependencies on each host
   â””â”€ Create systemd service files (or Windows services)

3. COMPONENT CONFIGURATION
   â”œâ”€ Generate component-specific .env files
   â”‚  - MDM_ADDRESS (all components)
   â”‚  - Component role, ports, paths
   â”‚  - Cluster name, node_id
   â”œâ”€ Create storage directories
   â”‚  - SDS: vm_storage/sds-<id>/
   â”‚  - SDC: vm_storage/sdc-<id>/
   â””â”€ Set filesystem permissions

4. SERVICE STARTUP (phased)
   â”œâ”€ Phase 1: Start MDM
   â”‚  â””â”€ Wait for MDM API health check
   â”œâ”€ Phase 2: Start all SDS nodes
   â”‚  â””â”€ Each SDS registers with MDM
   â”‚  â””â”€ Wait for all SDS healthy
   â”œâ”€ Phase 3: Start all SDC nodes
   â”‚  â””â”€ Each SDC registers with MDM
   â”‚  â””â”€ Wait for all SDC healthy
   â””â”€ Phase 4: Start MGMT
       â””â”€ MGMT discovers topology from MDM
       â””â”€ GUI accessible

5. CLUSTER INITIALIZATION
   â”œâ”€ Create protection domains from topology YAML
   â”œâ”€ Assign SDS nodes to fault sets
   â”œâ”€ Create storage pools
   â”œâ”€ Verify chunk placement works
   â””â”€ Create default admin user in MGMT

6. HEALTH VALIDATION
   â”œâ”€ All components registered in MDM
   â”œâ”€ All heartbeats green
   â”œâ”€ Topology matches YAML
   â””â”€ Run smoke test: create vol, map, write, read, unmap, delete

7. ROLLBACK (if failure)
   â”œâ”€ Stop all services
   â”œâ”€ Remove installed files
   â””â”€ Report failure reason
```

### 11.5 Component Service Management

**Systemd service template** (Linux):

```ini
# /etc/systemd/system/powerflex-sds.service
[Unit]
Description=PowerFlex SDS Node
After=network.target

[Service]
Type=simple
User=powerflex
WorkingDirectory=/opt/powerflex
Environment="POWERFLEX_MDM_ADDRESS=10.0.1.1:8001"
Environment="POWERFLEX_SDS_NODE_ID=sds-1"
Environment="POWERFLEX_SDS_CONTROL_PORT=9100"
Environment="POWERFLEX_SDS_DATA_PORT=9700"
Environment="POWERFLEX_SDS_MGMT_PORT=9200"
ExecStart=/opt/powerflex/.venv/bin/python -m sds.run
Restart=always
RestartSec=10
StandardOutput=journal
StandardError=journal

[Install]
WantedBy=multi-user.target
```

**Windows service wrapper** (via `pywin32` or `nssm`):
```powershell
# Install SDS as Windows service
nssm install PowerFlex-SDS "C:\PowerFlex\.venv\Scripts\python.exe" "-m sds.run"
nssm set PowerFlex-SDS AppDirectory "C:\PowerFlex"
nssm set PowerFlex-SDS AppEnvironmentExtra "POWERFLEX_MDM_ADDRESS=10.0.1.1:8001" "POWERFLEX_SDS_NODE_ID=sds-1"
nssm start PowerFlex-SDS
```

### 11.6 SSH/Remote Execution Helpers

**`scripts/remote_exec.py`** â€” Cross-platform remote command execution:

```python
import paramiko
from typing import List, Dict

class RemoteHost:
    def __init__(self, host: str, user: str, key_path: str):
        self.ssh = paramiko.SSHClient()
        self.ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
        self.ssh.connect(host, username=user, key_filename=key_path)
    
    def exec(self, command: str) -> tuple[int, str, str]:
        stdin, stdout, stderr = self.ssh.exec_command(command)
        exit_code = stdout.channel.recv_exit_status()
        return exit_code, stdout.read().decode(), stderr.read().decode()
    
    def upload(self, local_path: str, remote_path: str):
        sftp = self.ssh.open_sftp()
        sftp.put(local_path, remote_path)
        sftp.close()
    
    def close(self):
        self.ssh.close()
```

### 11.7 Deployment Workflow Example

```bash
# 1. Prepare topology config
cat > cluster_topology.yaml <<EOF
cluster_name: test_cluster
mdm_address: 192.168.1.10:8001
components:
  mdm: [{host: 192.168.1.10}]
  sds:
    - {host: 192.168.1.11, node_id: sds-1, capacity_gb: 256}
    - {host: 192.168.1.12, node_id: sds-2, capacity_gb: 256}
  sdc:
    - {host: 192.168.1.20, node_id: sdc-1}
  mgmt: [{host: 192.168.1.100}]
protection_domains:
  - name: PD1
storage_pools:
  - {name: POOL1, pd: PD1, capacity_gb: 512}
EOF

# 2. Deploy cluster
python scripts/deploy_cluster.py \
  --mode distributed \
  --config cluster_topology.yaml \
  --ssh-user powerflex \
  --ssh-key ~/.ssh/powerflex_rsa \
  --venv-path /opt/powerflex/.venv

# Output:
# [PRE-FLIGHT] Checking connectivity...
#   âœ“ 192.168.1.10 reachable
#   âœ“ 192.168.1.11 reachable
#   âœ“ 192.168.1.12 reachable
#   âœ“ 192.168.1.20 reachable
#   âœ“ 192.168.1.100 reachable
# [PRE-FLIGHT] Checking ports...
#   âœ“ No conflicts detected
# [INSTALL] Copying packages...
#   âœ“ mdm â†’ 192.168.1.10
#   âœ“ sds â†’ 192.168.1.11
#   âœ“ sds â†’ 192.168.1.12
#   âœ“ sdc â†’ 192.168.1.20
#   âœ“ mgmt â†’ 192.168.1.100
# [INSTALL] Creating venv...
#   âœ“ All hosts ready
# [CONFIG] Generating .env files...
#   âœ“ All configs written
# [STARTUP] Starting MDM...
#   âœ“ MDM API healthy at http://192.168.1.10:8001
# [STARTUP] Starting SDS nodes...
#   âœ“ sds-1 registered
#   âœ“ sds-2 registered
# [STARTUP] Starting SDC nodes...
#   âœ“ sdc-1 registered
# [STARTUP] Starting MGMT...
#   âœ“ MGMT GUI at http://192.168.1.100:5000
# [INIT] Creating cluster topology...
#   âœ“ Protection domain PD1 created
#   âœ“ Storage pool POOL1 created (512 GB)
# [VALIDATE] Running smoke test...
#   âœ“ Volume create/map/write/read/unmap/delete OK
# [SUCCESS] Cluster test_cluster deployed and validated

# 3. Scale out: add 2 more SDS nodes
python scripts/deploy_cluster.py \
  --mode add \
  --component sds \
  --hosts 192.168.1.13,192.168.1.14 \
  --mdm-url http://192.168.1.10:8001 \
  --ssh-user powerflex \
  --ssh-key ~/.ssh/powerflex_rsa
```

### 11.8 Cluster Management Scripts

Additional helper scripts in `scripts/`:

```
scripts/
â”œâ”€â”€ deploy_cluster.py           â† Master deployment orchestrator
â”œâ”€â”€ remote_exec.py              â† SSH/remote helpers
â”œâ”€â”€ cluster_status.py           â† Health check all components
â”œâ”€â”€ cluster_stop.py             â† Graceful shutdown entire cluster
â”œâ”€â”€ cluster_start.py            â† Start all components in order
â”œâ”€â”€ cluster_restart.py          â† Rolling restart
â”œâ”€â”€ component_add.py            â† Add single component to cluster
â”œâ”€â”€ component_remove.py         â† Decommission component gracefully
â”œâ”€â”€ upgrade_component.py        â† In-place upgrade with zero-downtime
â”œâ”€â”€ backup_metadata.py          â† Backup all .db files from all VMs
â”œâ”€â”€ restore_metadata.py         â† Restore from backup
â””â”€â”€ validate_cluster.py         â† Full integration test suite
```

### 11.9 Environment Variable Template

**`.env.template`** â€” Component config blueprint:

```bash
# MDM component
POWERFLEX_COMPONENT_ROLE=MDM
POWERFLEX_MDM_API_PORT=8001
POWERFLEX_CLUSTER_NAME=my_cluster
POWERFLEX_DB_PATH=/opt/powerflex/data/powerflex.db

# SDS component
POWERFLEX_COMPONENT_ROLE=SDS
POWERFLEX_SDS_NODE_ID=sds-1
POWERFLEX_SDS_CONTROL_PORT=9100
POWERFLEX_SDS_DATA_PORT=9700
POWERFLEX_SDS_MGMT_PORT=9200
POWERFLEX_SDS_STORAGE_ROOT=/opt/powerflex/vm_storage/sds-1
POWERFLEX_MDM_ADDRESS=10.0.1.1:8001
POWERFLEX_DB_PATH=/opt/powerflex/data/sds_local.db

# SDC component
POWERFLEX_COMPONENT_ROLE=SDC
POWERFLEX_SDC_NODE_ID=sdc-1
POWERFLEX_SDC_CONTROL_PORT=8003
POWERFLEX_SDC_DEVICE_PORT=8005
POWERFLEX_SDC_MGMT_PORT=8004
POWERFLEX_MDM_ADDRESS=10.0.1.1:8001
POWERFLEX_DB_PATH=/opt/powerflex/data/sdc_chunks.db

# MGMT component
POWERFLEX_COMPONENT_ROLE=MGMT
POWERFLEX_MGMT_GUI_PORT=5000
POWERFLEX_MDM_ADDRESS=10.0.1.1:8001
POWERFLEX_DB_PATH=/opt/powerflex/data/mgmt.db
POWERFLEX_SECRET_KEY=<random-secret-for-sessions>
```

Deploy script generates these from `cluster_topology.yaml` and injects them into each VM at deployment time.

---

## 12. Technical Debt & Known Issues

### 12.1 SQLAlchemy Column Type Errors (85+ occurrences)
**Status:** Non-functional, deferred to Phase 10  
**Files:** `mdm/api/discovery.py`, `mdm/token_authority.py`, `mdm/api/token.py`  
**Issue:** Pylance type checker reports `Column[T]` vs `T` type mismatches  
**Examples:**
```python
# Pylance complains but code works:
existing.status = "ACTIVE"  # Cannot assign str to Column[str]
return config.value if config else None  # Cannot return Column[str] as str
```

**Root Cause:** SQLAlchemy's type stubs declare model attributes as `Column[T]` but at runtime they behave as `T`. This is a known SQLAlchemy typing limitation.

**Resolution Plan (Phase 10):**
- Option 1: Add `# type: ignore` comments (quick fix)
- Option 2: Use SQLAlchemy 2.0+ `Mapped[T]` typing (proper fix, requires migration)
- Option 3: Configure Pylance to ignore these specific errors
- **Recommended:** Option 2 - Migrate to SQLAlchemy 2.0 `Mapped[T]` pattern during Phase 10

### 12.2 Test Code Issues (Phase 4)
**Status:** Non-critical, fix when updating tests  
**File:** `scripts/test_phase4_tokens.py`  
**Issues:**
- Line 148: Type checking on `None` in string operator
- Line 243: Unreachable except clause  
- Lines 217-218: Undefined variables in test setup

**Resolution:** Clean up during Phase 10 integration testing

### 12.3 Token Verifier Type Safety (Phase 5)
**Status:** Low priority, works at runtime  
**File:** `sds/token_verifier.py` lines 93-111  
**Issue:** Token dict fields typed as `Unknown | None` instead of explicit types

**Resolution Plan:**
```python
# Current:
token_id = token.get("token_id")  # type: Unknown | None

# Better (add type guards):
token_id = token.get("token_id")
if not isinstance(token_id, str):
    return False, "Invalid token_id type"
```

Can be improved during Phase 10 or when adding comprehensive error handling.

---

## 13. Key Design Decisions (Updated)

1. **MDM is the only writer to powerflex.db.** No other component touches the central DB.

2. **MGMT has its own separate database.** Users, sessions, alerts, and monitoring data live in `mgmt.db`. MGMT never writes to `powerflex.db`.

3. **Every IO transaction requires a token.** SDC gets a signed token from MDM before any read/write. SDS verifies the token. SDS ACKs the completed transaction back to MDM. No token = no disk access.

4. **SDC serves volumes via NBD-like TCP protocol.** Apps connect to SDC port 8005, send framed JSON read/write commands. SDC handles token acquisition, chunk splitting, SDS communication, and ACK aggregation transparently.

5. **IO execution belongs in SDC only.** MDM generates plans and tokens, SDS stores bytes, SDC orchestrates the IO. MDM never reads/writes volume data.

6. **All components are independently deployable.** Each VM gets one component package + shared utils. No shared DB files, no shared memory. All communication over TCP/HTTP.

7. **MDM is the discovery registry.** Components register on boot via HTTP. MGMT discovers peers through MDM topology API.

8. **Cluster secret enables zero-trust IO.** HMAC-SHA256 tokens prevent unauthorized SDS access. Tokens are short-lived, single-use, and auditable.

9. **MGMT monitors ALL components through their mgmt plane.** Direct HTTP polling to each SDS/SDC/MDM mgmt port. No dependency on MDM for health data.

10. **Co-location is supported but not required.** Same codebase runs on 4 VMs or 1 VM with different ports. Socket-based communication makes this seamless.
